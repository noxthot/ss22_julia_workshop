<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/ss22_julia_workshop/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/ss22_julia_workshop/libs/highlight/github.min.css">
   
    <script src="/ss22_julia_workshop/libs/clipboard.min.js"></script>
  
  
  <script src="/ss22_julia_workshop/libs/plotly-1_58_5.min.js"></script> 
  <script>
    // This function is used when calling `\fig{...}` See # Using \fig{...} below
    const PlotlyJS_json = async (div, url) => {
      response = await fetch(url); // get file
      fig = await response.json(); // convert it to json
      // Make the plot fit the screen responsively. See the documentation of plotly.js. https://plotly.com/javascript/responsive-fluid-layout/
      if (typeof fig.config === 'undefined') { fig["config"]={} }
      delete fig.layout.width
      delete fig.layout.height
      fig["layout"]["autosize"] = true
      fig["config"]["autosizable"] = true
      fig["config"]["responsive"] = true

      // make it easier to scroll throught the website rather than being blocked by a figure.
      fig.config["scrollZoom"] = false

      // PlotlyJS.savefig by default add the some more attribute to make a static plot.
      // Disable them to make the website fancier.
      delete fig.config.staticPlot
      delete fig.config.displayModeBar
      delete fig.config.doubleClick
      delete fig.config.showTips

      Plotly.newPlot(div, fig);
    };
  </script>
  
  <link rel="stylesheet" href="/ss22_julia_workshop/css/jtd.css">
<link rel="stylesheet" href="/ss22_julia_workshop/css/extras.css">
<link rel="icon" href="/ss22_julia_workshop/assets/favicon.ico">

<style>
  /* #148 wrap long header */
  .franklin-content a.header-anchor,
  .franklin-toc li a
   {
    word-wrap: break-word;
    white-space: normal;
  }
</style>

   <title>Parallel Computing - GPU computing</title>  
</head>
<body>                      <!-- closed in foot.html -->
<div class="page-wrap">   <!-- closed in foot.html -->
  <!-- SIDE BAR -->
  <div class="side-bar">
    <div class="header">
      <a href="/ss22_julia_workshop/" class="title">
        Julia
      </a>
    </div>
    <label for="show-menu" class="show-menu">MENU</label>
    <input type="checkbox" id="show-menu" role="button">
    <div class="menu" id="side-menu">
      <ul class="menu-list">
        <li class="menu-list-item "><a href="/ss22_julia_workshop/" class="menu-list-link ">Start</a>
        <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/" class="menu-list-link ">Introduction</a>
          <ul class="menu-list-child-list ">
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/basis_datatypes_and_operations" class="menu-list-link">Basic Datatypes and Operations</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/package_manager" class="menu-list-link">Package Manager</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/matrix_vectors" class="menu-list-link">Matrix and Vector Operations</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/conditional_evaluations" class="menu-list-link">Conditional evaluations</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/loops" class="menu-list-link">Loops</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/functions" class="menu-list-link">Functions</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/worksheet_1" class="menu-list-link">Worksheet 1</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/introduction/worksheet_2" class="menu-list-link">Worksheet 2</a>
          </ul>
        <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/datascience/" class="menu-list-link ">Data Science</a>
          <ul class="menu-list-child-list ">
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/datascience/loading_data" class="menu-list-link">Loading data</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/datascience/saving_data" class="menu-list-link">Saving data</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/datascience/exploratory_da" class="menu-list-link">Exploratory Data Analysis</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/datascience/unsupervised_learning" class="menu-list-link">Unsupervised Learning</a>
          </ul>
        <li class="menu-list-item active"><a href="/ss22_julia_workshop/pages/hpc/" class="menu-list-link active">Parallel computing</a>
          <ul class="menu-list-child-list ">
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/hpc/performance" class="menu-list-link">Measuring performance</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/hpc/simd" class="menu-list-link">Single Instruction Multiple Data</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/hpc/pi" class="menu-list-link">&pi; example</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/hpc/multithreading" class="menu-list-link">Multithreading</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/hpc/distributed" class="menu-list-link">Distributed computing</a>
            <li class="menu-list-item "><a href="/ss22_julia_workshop/pages/hpc/gpu" class="menu-list-link">GPU computing</a>
          </ul>
      </ul>
    </div>
    <div class="footer">
      This is <em>Just the docs</em>, adapted from the <a href="https://github.com/pmarsceill/just-the-docs" target="_blank">Jekyll theme</a>.
    </div>
  </div>
  <!-- CONTENT -->
  <div class="main-content-wrap"> <!-- closed in foot.html -->
    <div class="main-content">    <!-- closed in foot.html -->
      <div class="main-header">
        SS22 Julia Workshop Obergurgl
      </div>



<!-- Content appended here (in class franklin-content) -->
<div class="franklin-content"><h1 id="gpu_computing_in_julia"><a href="#gpu_computing_in_julia" class="header-anchor">GPU computing in Julia</a></h1>
<p>A Graphic Processing Unit, or GPU for short, was originally designed to manipulate images an a frame buffer. Their inherent parallelism makes them more efficient for some tasks than CPUs.  Basically everything that is related to SIMD operations but not limited to them.   Using GPUs for general purpose computing &#40;GPGPU&#41; became a thing in the 21st century.  NVidia became the first big vendor that started to support this kind of application for their GPUs and invested heavily in dedicated frameworks to aid general purpose computing and later also started to produce dedicated hardware for this purpose only.  Nowadays, GPUs are usd for AI, deep learning and a lot of HPC workloads.</p>
<p>In Julia GPUs are supported by the  <a href="https://juliagpu.org/">JuliaGPU</a> project.  They support the t three big vendor frameworks: </p>
<ul>
<li><p>NVidia with <a href="https://docs.nvidia.com/cuda/">CUDA</a> and <a href="https://cuda.juliagpu.org/stable/"><code>CUDA.jl</code></a></p>
</li>
<li><p>AMD with <a href="https://rocmdocs.amd.com/en/latest/">ROCm</a> and <a href="https://github.com/JuliaGPU/AMDGPU.jl"><code>AMDGPU.jl</code></a></p>
</li>
<li><p>Intel with <a href="https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html">oneAPI</a> and <a href="https://github.com/JuliaGPU/oneAPI.jl"><code>oneAPI</code></a></p>
</li>
</ul>
<p>where <code>CUDA.jl</code> comes with the most features.  Nevertheless, in good Julia practice, the team behind JuliaGPU also included an abstraction layer, such that a lot of common functionality can be implemented without the need to specify a vendor.</p>

<figure style="text-align:center;">
<img src="/ss22_julia_workshop/assets/pages/hpc/GPUBackend.png" style="padding:0; " alt=" Compile strategy for JuliaGPU <br>Original source: https://www.youtube.com/watch?v=Hz9IMJuW5hU"/>
<figcaption> Compile strategy for JuliaGPU <br>Original source: https://www.youtube.com/watch?v=Hz9IMJuW5hU</figcaption>
</figure>

<p>We will focus on <code>CUDA.jl</code> as the card at hand is CUDA compatible. </p>
<p>We install tha package with the usual command and execute the test right away, to see if the GPU is working:</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-keyword">using</span> CUDA
</span>
<span class=hljs-metap>(@v1.7) pkg&gt;</span> test CUDA
┌ Info: System information:
│ CUDA toolkit 11.7, artifact installation
│ NVIDIA driver 510.73.5, for CUDA 11.6
│ CUDA driver 11.6
│ 
│ Libraries: 
│ - CUBLAS: 11.10.1
│ - CURAND: 10.2.10
│ - CUFFT: 10.7.2
│ - CUSOLVER: 11.3.5
│ - CUSPARSE: 11.7.3
│ - CUPTI: 17.0.0
│ - NVML: 11.0.0+510.73.5
│ - CUDNN: 8.30.2 (for CUDA 11.5.0)
│ - CUTENSOR: 1.4.0 (for CUDA 11.5.0)
│ 
│ Toolchain:
│ - Julia: 1.7.3
│ - LLVM: 12.0.1
│ - PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0
│ - Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80
│ 
│ 1 device:
└   0: NVIDIA GeForce RTX 3070 Laptop GPU (sm_86, 7.787 GiB / 8.000 GiB available)
Test Summary: |  Pass  Broken  Total
  Overall     | 17002       5  17007
    SUCCESS
     Testing CUDA tests passed</code></pre>
<p>The full output is in the example block to preserve a bit of readability.</p>
<button type="button" class="collapsible" style="background-color:#caffa5"> Example </button><div class="collapsiblecontent">  </p>
<pre><code class="julia-repl hljs"><span class=hljs-metap>(@v1.7) pkg&gt;</span> test CUDA
┌ Info: System information:
│ CUDA toolkit 11.7, artifact installation
│ NVIDIA driver 510.73.5, for CUDA 11.6
│ CUDA driver 11.6
│ 
│ Libraries: 
│ - CUBLAS: 11.10.1
│ - CURAND: 10.2.10
│ - CUFFT: 10.7.2
│ - CUSOLVER: 11.3.5
│ - CUSPARSE: 11.7.3
│ - CUPTI: 17.0.0
│ - NVML: 11.0.0+510.73.5
│ - CUDNN: 8.30.2 (for CUDA 11.5.0)
│ - CUTENSOR: 1.4.0 (for CUDA 11.5.0)
│ 
│ Toolchain:
│ - Julia: 1.7.3
│ - LLVM: 12.0.1
│ - PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0
│ - Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80
│ 
│ 1 device:
└   0: NVIDIA GeForce RTX 3070 Laptop GPU (sm_86, 7.787 GiB / 8.000 GiB available)
[ Info: Testing using 1 device(s): 0. NVIDIA GeForce RTX 3070 Laptop GPU (UUID bfdb6f60-dbcf-1a82-ca79-1c8b947c3f35)
                                                  |          | ---------------- GPU ---------------- | ---------------- CPU ---------------- |
Test                                     (Worker) | Time (s) | GC (s) | GC % | Alloc (MB) | RSS (MB) | GC (s) | GC % | Alloc (MB) | RSS (MB) |
Test                                     (Worker) | Time (s) | GC (s) | GC % | Alloc (MB) | RSS (MB) | GC (s) | GC % | Alloc (MB) | RSS (MB) |
initialization                                (2) |     5.64 |   0.00 |  0.0 |       0.00 |   165.00 |   0.08 |  1.4 |     512.27 |   843.59 |
gpuarrays/indexing scalar                     (2) |    25.43 |   0.00 |  0.0 |       0.01 |   181.00 |   1.20 |  4.7 |    4688.43 |   843.59 |
gpuarrays/reductions/reducedim!               (2) |    90.88 |   0.00 |  0.0 |       1.03 |   183.00 |   8.52 |  9.4 |   22482.18 |  1163.61 |
gpuarrays/linalg                              (2) |    54.46 |   0.00 |  0.0 |      11.59 |   551.00 |   3.43 |  6.3 |   10524.64 |  2332.89 |
gpuarrays/math/power                          (2) |    35.13 |   0.00 |  0.0 |       0.01 |   551.00 |   3.23 |  9.2 |    8032.86 |  2492.46 |
gpuarrays/linalg/mul!/vector-matrix           (2) |    55.05 |   0.00 |  0.0 |       0.02 |   551.00 |   3.45 |  6.3 |   11596.70 |  2833.54 |
gpuarrays/indexing multidimensional           (2) |    37.84 |   0.00 |  0.0 |       1.21 |   183.00 |   2.40 |  6.4 |    7920.18 |  2833.54 |
gpuarrays/interface                           (2) |     4.45 |   0.00 |  0.0 |       0.00 |   181.00 |   0.31 |  6.9 |     858.64 |  2833.54 |
gpuarrays/reductions/any all count            (2) |    16.45 |   0.00 |  0.0 |       0.00 |   181.00 |   1.46 |  8.9 |    4293.68 |  2833.54 |
gpuarrays/reductions/minimum maximum extrema  (2) |   140.12 |   0.01 |  0.0 |       1.41 |   185.00 |  10.18 |  7.3 |   31924.33 |  2954.52 |
gpuarrays/uniformscaling                      (2) |     8.35 |   0.00 |  0.0 |       0.01 |   181.00 |   0.41 |  4.9 |    1288.85 |  2954.52 |
gpuarrays/linalg/mul!/matrix-matrix           (2) |   107.47 |   0.01 |  0.0 |       0.12 |   553.00 |   6.31 |  5.9 |   19136.07 |  4212.98 |
gpuarrays/math/intrinsics                     (2) |     3.74 |   0.00 |  0.0 |       0.00 |   181.00 |   0.19 |  5.0 |     711.11 |  4212.98 |
gpuarrays/linalg/norm                         (2) |   268.94 |   0.01 |  0.0 |       0.02 |   257.00 |  21.18 |  7.9 |   52430.53 |  5626.27 |
gpuarrays/statistics                          (2) |    77.79 |   0.00 |  0.0 |       1.51 |   551.00 |   4.21 |  5.4 |   13876.69 |  6655.55 |
gpuarrays/reductions/mapreduce                (2) |   203.11 |   0.01 |  0.0 |       1.81 |   185.00 |  11.73 |  5.8 |   37609.42 |  6729.29 |
gpuarrays/constructors                        (2) |    19.20 |   0.00 |  0.0 |       0.08 |   181.00 |   0.68 |  3.5 |    2569.40 |  6729.29 |
gpuarrays/random                              (2) |    28.17 |   0.00 |  0.0 |       0.03 |   181.00 |   1.52 |  5.4 |    4786.98 |  6729.29 |
gpuarrays/base                                (2) |    26.12 |   0.00 |  0.0 |       8.82 |   181.00 |   1.37 |  5.2 |    4506.52 |  6816.72 |
gpuarrays/reductions/== isequal               (2) |    98.03 |   0.01 |  0.0 |       1.07 |   183.00 |  10.38 | 10.6 |   16597.95 |  7809.12 |
gpuarrays/broadcasting                        (2) |   362.62 |   0.01 |  0.0 |       2.00 |   185.00 |  42.96 | 11.8 |   53772.92 |  8699.58 |
gpuarrays/reductions/mapreducedim!            (2) |    56.83 |   0.00 |  0.0 |       1.54 |   183.00 |   6.86 | 12.1 |    7394.71 |  8699.58 |
gpuarrays/reductions/reduce                   (2) |    19.58 |   0.00 |  0.0 |       1.21 |   183.00 |   0.44 |  2.3 |    2226.51 |  8699.58 |
gpuarrays/reductions/sum prod                 (2) |   279.16 |   0.01 |  0.0 |       3.24 |   185.00 |  34.08 | 12.2 |   40919.47 | 10091.04 |
apiutils                                      (2) |     0.11 |   0.00 |  0.0 |       0.00 |   165.00 |   0.00 |  0.0 |       0.81 | 10091.04 |
array                                         (2) |   153.18 |   0.01 |  0.0 |    1266.37 |  1341.00 |  36.18 | 23.6 |   19461.83 | 10680.45 |
broadcast                                     (2) |    22.37 |   0.00 |  0.0 |       0.00 |  1333.00 |   3.60 | 16.1 |    2901.68 | 10680.45 |
codegen                                       (2) |    10.09 |   0.00 |  0.0 |       0.00 |  1393.00 |   1.39 | 13.7 |    1456.06 | 10680.45 |
cublas                                        (2) |    83.06 |   0.02 |  0.0 |      14.55 |  1717.00 |   9.41 | 11.3 |   12030.98 | 11289.79 |
cudadrv                                       (2) |     6.99 |   0.00 |  0.0 |       0.00 |  1335.00 |   0.28 |  4.1 |     791.80 | 11308.88 |
cufft                                         (2) |    24.73 |   0.00 |  0.0 |     233.38 |  1437.00 |   2.91 | 11.8 |    2843.60 | 11314.30 |
curand                                        (2) |     0.17 |   0.00 |  0.0 |       0.00 |  1327.00 |   0.09 | 50.0 |       3.75 | 11314.30 |
cusparse                                      (2) |    49.55 |   0.02 |  0.0 |      10.81 |  1491.00 |   2.72 |  5.5 |    5858.52 | 11459.28 |
examples                                      (2) |    85.06 |   0.00 |  0.0 |       0.00 |  1317.00 |   0.00 |  0.0 |      45.06 | 11459.28 |
exceptions                                    (2) |    53.14 |   0.00 |  0.0 |       0.00 |  1317.00 |   0.00 |  0.0 |       1.57 | 11459.28 |
execution                                        (2) |    94.27 |   0.00 |  0.0 |       0.49 |  1401.00 |  21.67 | 23.0 |   11410.60 | 11459.28 |
iterator                                      (2) |     2.11 |   0.00 |  0.0 |       1.93 |  1317.00 |   0.00 |  0.0 |     360.93 | 11459.28 |
linalg                                        (2) |    38.90 |   0.00 |  0.0 |       9.03 |  1405.00 |   8.86 | 22.8 |    5066.35 | 11459.28 |
nvml                                          (2) |     0.51 |   0.00 |  0.0 |       0.00 |  1317.00 |   0.00 |  0.0 |      28.69 | 11459.28 |
nvtx                                          (2) |     0.23 |   0.00 |  0.0 |       0.00 |  1317.00 |   0.00 |  0.0 |      34.94 | 11459.28 |
pointer                                       (2) |     0.25 |   0.00 |  0.0 |       0.00 |  1317.00 |   0.00 |  0.0 |      11.54 | 11459.28 |
pool                                          (2) |     4.44 |   0.00 |  0.0 |       0.00 |   165.00 |   2.79 | 62.9 |     244.88 | 11459.28 |
random                                        (2) |    31.17 |   0.00 |  0.0 |     256.58 |   445.00 |   6.35 | 20.4 |    3519.72 | 11459.28 |
sorting                                       (2) |   227.27 |   0.01 |  0.0 |     543.84 |  1777.00 |  50.72 | 22.3 |   28578.54 | 13741.85 |
texture                                       (2) |    54.81 |   0.00 |  0.0 |       0.09 |   443.00 |   8.27 | 15.1 |    7309.86 | 13741.85 |
threading                                     (2) |     2.94 |   0.00 |  0.0 |      10.94 |   811.00 |   0.45 | 15.5 |     299.54 | 13741.85 |
utils                                         (2) |     0.80 |   0.00 |  0.0 |       0.00 |   421.00 |   0.00 |  0.0 |      76.69 | 13741.85 |
cudnn/activation                              (2) |     1.83 |   0.00 |  0.0 |       0.00 |   545.00 |   0.14 |  7.4 |     186.83 | 13741.85 |
cudnn/convolution                             (2) |     0.07 |   0.00 |  0.0 |       0.00 |   421.00 |   0.00 |  0.0 |       6.14 | 13741.85 |
cudnn/dropout                                 (2) |     1.37 |   0.00 |  0.0 |       1.43 |   549.00 |   0.22 | 16.1 |      88.64 | 13741.85 |
cudnn/inplace                                 (2) |     0.85 |   0.00 |  0.0 |       0.01 |   549.00 |   0.19 | 22.4 |      50.14 | 13741.85 |
cudnn/multiheadattn                           (2) |    12.14 |   0.00 |  0.0 |       0.15 |   927.00 |   1.91 | 15.7 |    1592.27 | 13741.85 |
cudnn/normalization                           (2) |    15.76 |   0.00 |  0.0 |       0.08 |   589.00 |   1.66 | 10.6 |    1682.89 | 13766.65 |
cudnn/optensor                                (2) |     1.24 |   0.00 |  0.0 |       0.00 |   545.00 |   0.13 | 10.7 |     136.46 | 13766.79 |
cudnn/pooling                                 (2) |     4.49 |   0.00 |  0.0 |       0.06 |   545.00 |   0.17 |  3.8 |     666.79 | 13766.79 |
cudnn/reduce                                  (2) |     1.42 |   0.00 |  0.0 |       0.02 |   545.00 |   0.14 | 10.0 |     207.65 | 13766.79 |
cudnn/rnn                                     (2) |     6.24 |   0.00 |  0.1 |     898.13 |  1567.00 |   0.22 |  3.4 |     649.49 | 13865.52 |
cudnn/softmax                                 (2) |     0.98 |   0.00 |  0.0 |       0.01 |  1185.00 |   0.12 | 12.5 |      73.36 | 13865.70 |
cudnn/tensor                                  (2) |     1.99 |   0.00 |  0.0 |       0.00 |   175.00 |   1.65 | 83.0 |      25.64 | 13865.70 |
cusolver/dense                                (2) |   136.80 |   0.04 |  0.0 |    1467.10 |   617.00 |  29.02 | 21.2 |   15502.48 | 14179.64 |
cusolver/multigpu                             (2) |     7.68 |   0.00 |  0.0 |     545.89 |   979.00 |   0.19 |  2.5 |     991.26 | 14179.64 |
cusolver/sparse                               (2) |     5.29 |   0.00 |  0.0 |       0.18 |   721.00 |   0.37 |  6.9 |     420.74 | 14179.64 |
cusparse/broadcast                            (2) |    75.65 |   0.00 |  0.0 |       0.02 |   617.00 |  14.20 | 18.8 |    8232.49 | 14371.40 |
cusparse/conversions                          (2) |     8.84 |   0.00 |  0.0 |       0.02 |   615.00 |   0.50 |  5.7 |    1089.83 | 14371.40 |
cusparse/device                               (2) |     0.32 |   0.00 |  0.0 |       0.01 |   615.00 |   0.11 | 34.3 |       3.66 | 14371.40 |
cusparse/generic                              (2) |     1.85 |   0.00 |  0.0 |       0.05 |   615.00 |   0.11 |  5.9 |     182.92 | 14371.40 |
cusparse/interfaces                           (2) |    34.24 |   0.01 |  0.0 |       0.97 |   615.00 |   3.63 | 10.6 |    3770.70 | 14371.40 |
cusparse/linalg                               (2) |     4.44 |   0.00 |  0.0 |       0.01 |   615.00 |   0.10 |  2.2 |     560.27 | 14371.40 |
cutensor/base                                 (2) |     0.11 |   0.00 |  0.0 |       0.05 |   517.00 |   0.00 |  0.0 |      14.68 | 14371.40 |
cutensor/contractions                         (2) |    22.99 |   0.01 |  0.0 |   10626.41 |  1911.00 |   1.12 |  4.9 |    3072.22 | 14597.90 |
cutensor/elementwise_binary                   (2) |    21.48 |   0.00 |  0.0 |       6.11 |  1427.00 |   0.78 |  3.6 |    2347.49 | 14597.90 |
cutensor/elementwise_trinary                  (2) |    26.38 |   0.00 |  0.0 |       2.71 |  1427.00 |   0.98 |  3.7 |    3145.61 | 14597.90 |
cutensor/permutations                         (2) |     2.94 |   0.00 |  0.0 |       1.36 |  1427.00 |   0.12 |  3.9 |     312.68 | 14597.90 |
cutensor/reductions                           (2) |    22.97 |   0.00 |  0.0 |      36.36 |  1427.00 |   0.42 |  1.8 |    2181.72 | 15127.08 |
device/array                                  (2) |     8.47 |   0.00 |  0.0 |       0.00 |  1333.00 |   0.33 |  3.9 |    1149.69 | 15127.08 |
device/intrinsics                             (2) |    56.15 |   0.00 |  0.0 |       0.00 |  2015.00 |  10.61 | 18.9 |    7190.85 | 15161.48 |
device/ldg                                    (2) |    10.23 |   0.00 |  0.0 |       0.00 |  1333.00 |   0.33 |  3.3 |    1598.80 | 15161.48 |
device/random                                 (2) |    70.45 |   0.00 |  0.0 |       0.17 |  1335.00 |  13.64 | 19.4 |    7687.06 | 15161.48 |
device/intrinsics/atomics                     (2) |   122.21 |   0.00 |  0.0 |       0.00 |  1335.00 |  24.14 | 19.7 |   13241.89 | 15169.65 |
device/intrinsics/math                        (2) |    87.89 |   0.00 |  0.0 |       0.00 |  1363.00 |  15.47 | 17.6 |    8747.68 | 15202.04 |
device/intrinsics/memory                      (2) |    33.16 |   0.00 |  0.0 |       0.02 |  1333.00 |   6.44 | 19.4 |    4266.41 | 15202.04 |
device/intrinsics/output                      (2) |    38.30 |   0.00 |  0.0 |       0.00 |  1333.00 |   8.02 | 20.9 |    4777.54 | 15202.04 |
device/intrinsics/wmma                        (2) |   132.81 |   0.01 |  0.0 |       0.63 |  1337.00 |  28.43 | 21.4 |   15981.42 | 15469.16 |
Testing finished in 1 hour, 10 minutes, 21 seconds, 161 milliseconds

Test Summary: |  Pass  Broken  Total
  Overall     | 17002       5  17007
    SUCCESS
     Testing CUDA tests passed</code></pre>
<p></div>
<p>This already provides us with a lot of information about the card and the supported CUDA library versions. Note that <code>CUDA.jl</code> will look up the driver &#40;the only requirement&#41; and download the best CUDA version on its own. Therefore, it is not necessary the same as you have installed. </p>
<p>Right away we are able to use the high level functionality of the <code>CUDA.jl</code> package with the <em>implicit parallelism programming model</em>.</p>
<p>Let us look at some examples:</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> a = CuArray([<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>,<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>])
</span>4-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
 1.0
 2.0
 3.0
 4.0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> a .+= <span class="hljs-number">1</span>
</span>4-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
 2.0
 3.0
 4.0
 5.0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> a .+= a
</span>4-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
  4.0
  6.0
  8.0
 10.0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> sum(a)
</span>28.0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> A = a * a&#x27;
</span>4×4 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:
 16.0  24.0  32.0   40.0
 24.0  36.0  48.0   60.0
 32.0  48.0  64.0   80.0
 40.0  60.0  80.0  100.0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> d = CUDA.rand(length(a))
</span>4-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
 0.2327924
 0.06764824
 0.7085522
 0.97067034

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-keyword">using</span> LinearAlgebra
</span>
<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> mul!(d, A, d)
</span>ERROR: ArgumentError: output matrix must not be aliased with input matrix
Stacktrace:
 [1] gemm_dispatch!(C::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, A::CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, B::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, alpha::Bool, beta::Bool)
   @ CUDA.CUBLAS ~/.julia/packages/CUDA/tTK8Y/lib/cublas/linalg.jl:269
 [2] gemv_dispatch!(Y::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, A::CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, B::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, alpha::Bool, beta::Bool)
   @ CUDA.CUBLAS ~/.julia/packages/CUDA/tTK8Y/lib/cublas/linalg.jl:181
 [3] mul!
   @ ~/.julia/packages/CUDA/tTK8Y/lib/cublas/linalg.jl:188 [inlined]
 [4] mul!(C::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, A::CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, B::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer})
   @ LinearAlgebra /opt/julia-1.7.3/share/julia/stdlib/v1.7/LinearAlgebra/src/matmul.jl:275
 [5] top-level scope
   @ REPL[23]:1
 [6] top-level scope
   @ ~/.julia/packages/CUDA/tTK8Y/src/initialization.jl:52

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> d = CUDA.rand(<span class="hljs-built_in">Float64</span>, length(a))
</span>4-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
 0.6985405365128223
 0.1389900111808831
 0.41880569903314474
 0.47366941788943956

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> mul!(d, A, d)
</span>4-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
  46.86096793718457
  70.29145190577685
  93.72193587436914
 117.15241984296142

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> qr(d)
</span>CUDA.CUSOLVER.CuQR{Float64, CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}} with factors Q and R:
[┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007f27b8a15150.
│ Invocation of CuQRPackedQ getindex resulted in scalar indexing of a GPU array.
│ This is typically caused by calling an iterating implementation of a method.
│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,
│ and therefore are only permitted from the REPL for prototyping purposes.
│ If you did intend to index this array, annotate the caller with @allowscalar.
└ @ GPUArraysCore ~/.julia/packages/GPUArraysCore/rSIl2/src/GPUArraysCore.jl:81
-0.2721655269759087 -0.40824829046386296 -0.5443310539518174 -0.6804138174397716; -0.40824829046386296 0.8689897948556636 -0.17468027352578192 -0.21835034190722735; -0.5443310539518174 -0.1746802735257819 0.7670929686322908 -0.29113378920963645; -0.6804138174397716 -0.21835034190722735 -0.2911337892096365 0.6360827634879545]
[-172.17819044853752;;]

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> broadcast(a) <span class="hljs-keyword">do</span> x sin(x) <span class="hljs-keyword">end</span>
</span>4-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
 -0.7568024953079282
 -0.27941549819892586
  0.9893582466233818
 -0.5440211108893699

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> reduce(+, a)
</span>28.0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> accumulate(+, a)
</span>4-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
  4.0
 10.0
 18.0
 28.0</code></pre>
<p>Already with this functionality we can code up a lot of problems in scientific computing and port it to the GPU without knowing anything else than how to include <code>CUDA.jl</code>.  It is worth noting, that this is also highly efficient, as the people behind <code>CUDA.jl</code> optimize the calls with the same features as Julia itself and they work extraordinary well. </p>
<p>Of course we can not express everything in these terms.  Sometimes it is necessary to write a specific CUDA kernel function &#40;kernel is the technical name given to a function that is executed on the GPU&#41;. So here is a possible implementation of our <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> example.</p>
<p>The main idea is to use the most of the GPU by performing as many operations in parallel as possible, this also means we need to represent <code>M</code> as a vector again.  The GPU at hand supports a maximum of <code>1024</code> threads, so let us use all of them. </p>
<p>There is also one additional thing to keep in mind, that is, a kernel can not return a value.  That means we need to give <code>M</code> as an input variable rather than an output variable and in order to stick to the convention in Julia we define the kernel as:</p>
<pre><code class="julia hljs"><span class="hljs-keyword">function</span> in_unit_circle_kernel!(N::<span class="hljs-built_in">Int64</span>, M)
    j =  threadIdx().x 
    
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:N
        <span class="hljs-keyword">if</span> (rand()^<span class="hljs-number">2</span> + rand()^<span class="hljs-number">2</span>) &lt; <span class="hljs-number">1</span>
            <span class="hljs-meta">@inbounds</span> M[j] += <span class="hljs-number">1</span>
        <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">return</span>
<span class="hljs-keyword">end</span></code></pre>
<p>This looks very similar to our function <a href="./multithreading#actually_distribute_the_work"><code>in_unit_circle_threaded3</code></a>.  Instead of <code>treadid&#40;&#41;</code> as we had it in multithreading, this time the id is queried by <code>threadIdx&#40;&#41;.x</code>.  The <code>.x</code> is due to the fact that we could also have two dimensional arrays &#40;remember GPUs where designed to work with images&#41;.</p>
<p>The kernel is is now called with the <code>@cuda</code> macro and the number of threads given as an argument.  Note, that we also define <code>M</code> to have dimension <code>1024</code> and <code>Int8</code> as type.</p>
<pre><code class="julia hljs"><span class="hljs-keyword">function</span> in_unit_circle_gpu(N::<span class="hljs-built_in">Int64</span>)
    len, _ = divrem(N, <span class="hljs-number">2</span>^<span class="hljs-number">10</span>)
    M = CUDA.zeros(<span class="hljs-built_in">Int8</span>, <span class="hljs-number">2</span>^<span class="hljs-number">10</span>)

    <span class="hljs-meta">@cuda</span> threads = <span class="hljs-number">1024</span> in_unit_circle_kernel!(len, M)

    <span class="hljs-keyword">return</span> sum(M)
<span class="hljs-keyword">end</span></code></pre>
<p>and the performance:</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> get_accuracy(in_unit_circle_gpu, N)
</span>  6.65503269949852e-5

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@btime</span> estimate_pi(in_unit_circle_gpu, N);
</span>  1.740 s (157 allocations: 8.25 KiB)</code></pre>
<p>We are already faster than the standard Julia implementation but this is not what we where hoping for. </p>
<p>What we did in the above example was to use the maximal number of threads &#40;<code>1024</code>&#41; support by the used GPU and performed out operations with them. This way we only occupied one <em>streaming multiprocessor</em> &#40;SM&#41; but the GPU has several SMs.  To get the full performance we need to run our kernel not just with multiple threads, but also with multiple blocks. </p>
<p>In this technical blog <a href="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/">CUDA Refresher: The CUDA Programming Model</a> we can read more about it. The following figure illustrates it quite nicelly. 
<figure style="text-align:center;">
<img src="/ss22_julia_workshop/assets/pages/hpc/kernel-execution-on-gpu-1.png" style="padding:0; " alt=" Kernel execution on GPU. <br>Original source: https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/"/>
<figcaption> Kernel execution on GPU. <br>Original source: https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/</figcaption>
</figure>
</p>
<p>So we end up with the following code:</p>
<pre><code class="julia hljs"><span class="hljs-keyword">function</span> in_unit_circle_kernel2!(M)
    j = (blockIdx().x - <span class="hljs-number">1</span>) * blockDim().x + threadIdx().x

    <span class="hljs-keyword">if</span> (rand()^<span class="hljs-number">2</span> + rand()^<span class="hljs-number">2</span>) &lt; <span class="hljs-number">1</span>
        <span class="hljs-meta">@inbounds</span> M[j] += <span class="hljs-number">1</span>
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">return</span>
<span class="hljs-keyword">end</span>

<span class="hljs-keyword">function</span> in_unit_circle_gpu2(N::<span class="hljs-built_in">Int64</span>)
    nthreads = <span class="hljs-number">2</span>^<span class="hljs-number">10</span>
    nblocks, _ = divrem(N, nthreads)
    M = CUDA.zeros(<span class="hljs-built_in">Int8</span>, N)

    <span class="hljs-meta">@cuda</span> threads = nthreads blocks = nblocks in_unit_circle_kernel2!(M)

    <span class="hljs-keyword">return</span> sum(M)
<span class="hljs-keyword">end</span></code></pre>
<p>and the performance:</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> get_accuracy(in_unit_circle_gpu2, N)
</span>  1.79504328450264e-5

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@btime</span> estimate_pi(in_unit_circle_gpu2, N);
</span>  81.436 ms (156 allocations: 8.16 KiB)</code></pre>
<p>Now each thread on the GPU is doing one computation. This is actually not the most efficient way to use the GPU and in additio it is rather wastefull on memory. </p>
<p>So let us optimize it a bit further. Each of the threads should do <code>n</code> iterations. This means we need to define the number of blocks as </p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>n</mi><mi>b</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi><mi>s</mi><mo>=</mo><mfrac><mi>N</mi><mrow><mi>n</mi><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mi>n</mi></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">
 nblocks = \frac{N}{nthreads n}.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">nb</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oc</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0463em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">re</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">.</span></span></span></span></span>
<p>The kernel is more or less the same but the loop returns.</p>
<pre><code class="julia hljs"><span class="hljs-keyword">function</span> in_unit_circle_kernel3!(n::<span class="hljs-built_in">Int64</span>, M)
    j = (blockIdx().x - <span class="hljs-number">1</span>) * blockDim().x + threadIdx().x

    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:n
        <span class="hljs-keyword">if</span> (rand()^<span class="hljs-number">2</span> + rand()^<span class="hljs-number">2</span>) &lt; <span class="hljs-number">1</span>
            <span class="hljs-meta">@inbounds</span> M[j] += <span class="hljs-number">1</span>
        <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">return</span>
<span class="hljs-keyword">end</span></code></pre>
<p>The calling function needs to compute the values as defined above. </p>
<pre><code class="julia hljs"><span class="hljs-keyword">function</span> in_unit_circle_gpu3(N::<span class="hljs-built_in">Int64</span>)
    nthreads = <span class="hljs-number">2</span>^<span class="hljs-number">10</span>
    n = <span class="hljs-number">2</span>^<span class="hljs-number">6</span>
    
    nblocks, _ = divrem(N, n * nthreads)
    M = CUDA.zeros(<span class="hljs-built_in">Int8</span>, nblocks * nthreads)
    
    <span class="hljs-meta">@cuda</span> threads = nthreads blocks = nblocks in_unit_circle_kernel3!(n, M)

    <span class="hljs-keyword">return</span> sum(M)
<span class="hljs-keyword">end</span></code></pre>
<p>and the performance is pretty good</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> get_accuracy(in_unit_circle_gpu2, N)
</span>  5.6093680210977936e-5

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@btime</span> estimate_pi(in_unit_circle_gpu2, N);
</span>  45.194 ms (155 allocations: 8.14 KiB)</code></pre>
<p>One thing you have to keep in mind, we definde <code>M</code> of type <code>Int8</code> to safe space and boost performance.  If <code>n</code> become larger than <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>7</mn></msup></mrow><annotation encoding="application/x-tex">2^7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">7</span></span></span></span></span></span></span></span></span></span></span> it might happen that the result is to large and can not longer be stored in an 8-Bit integer. </p>
<p>Like with the rest of the topics in this section we skimmed the surface and did not make a deep dive &#40;like memory access, multi GPU programming, multithreaded/distributed computing with GPUs and so forth&#41;.  It should give you a sound idea on what is happening and how great and easy you can accieve all of it with Julia.</p>
<div class="page-foot">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Gregor Ehrensperger, Peter Kandolf, Jonas Kusch. Last modified: July 09, 2022.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    </div> <!-- end of class main-content -->
    </div> <!-- end of class main-content-wrap -->
    </div> <!-- end of class page-wrap-->
    
      



    
    
      


      <script>
    (function(){
    
      // Get the elements.
      // - the 'pre' element.
      // - the 'div' with the 'paste-content' id.
    
      var pre = document.getElementsByTagName('pre');
    
      // Add a copy button in the 'pre' element.
      // which only has the className of 'language-'.
    
      for (var i = 0; i < pre.length; i++) {
        var isLanguage = pre[i].children[0].tagName == 'CODE';
    
        if ( isLanguage ) {
          var button           = document.createElement('button');
              button.className = 'copy-button';
              button.textContent = 'Copy';
    
              pre[i].appendChild(button);
        }
      };
    
      // Run Clipboard
    
      var copyCode = new Clipboard('.copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
    
      // On success:
      // - Change the "Copy" text to "Copied".
      // - Swap it to "Copy" in 2s.
      // - Lead user to the "contenteditable" area with Velocity scroll.
    
      copyCode.on('success', function(event) {
        event.clearSelection();
        event.trigger.textContent = 'Copied';
        window.setTimeout(function() {
          event.trigger.textContent = 'Copy';
        }, 2000);
    
      });
    
      // On error (Safari):
      // - Change the  "Press Ctrl+C to copy"
      // - Swap it to "Copy" in 2s.
    
      copyCode.on('error', function(event) {
        event.trigger.textContent = 'Press "Ctrl + C" to copy';
        window.setTimeout(function() {
          event.trigger.textContent = 'Copy';
        }, 5000);
      });
    
    })();
</script>
    
  </body>
</html>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;

  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.display === "block") {
        content.style.display = "none";
      } else {
        content.style.display = "block";
      }
    });
  }
</script>

<script>
  var coll = document.getElementsByClassName("solutioncollapsible");
  const queryString = window.location.search;
  const urlParams = new URLSearchParams(queryString);
  const myVar = urlParams.get('solution')
  if ( myVar == 'true') {
    for (i = 0; i < coll.length; i++) {
      coll[i].style.display = "block";
    }
  }
</script>